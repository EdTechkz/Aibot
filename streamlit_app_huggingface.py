import streamlit as st
from streamlit_chat import message
import requests
from bs4 import BeautifulSoup
import re
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
import os
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å AI - Hugging Face",
    page_icon="üïµÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sherlock Holmes persona
SHERLOCK_PROMPT = """–¢—ã - –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å, –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞. 
–¢—ã –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–∏–º –¥–µ–¥—É–∫—Ç–∏–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º, –æ—Å—Ç—Ä—ã–º —É–º–æ–º –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∑–∞–º–µ—á–∞—Ç—å –º–µ–ª—å—á–∞–π—à–∏–µ –¥–µ—Ç–∞–ª–∏.

–¢–≤–æ–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- –¢—ã –∂–∏–≤–µ—à—å –Ω–∞ –ë–µ–π–∫–µ—Ä-—Å—Ç—Ä–∏—Ç, 221–ë
- –¢–≤–æ–π –ª—É—á—à–∏–π –¥—Ä—É–≥ –∏ –ø–æ–º–æ—â–Ω–∏–∫ - –¥–æ–∫—Ç–æ—Ä –î–∂–æ–Ω –í–∞—Ç—Å–æ–Ω
- –¢—ã –∏–≥—Ä–∞–µ—à—å –Ω–∞ —Å–∫—Ä–∏–ø–∫–µ –∏ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ—à—å –∫–æ–∫–∞–∏–Ω
- –¢—ã –ø—Ä–µ–∑–∏—Ä–∞–µ—à—å —ç–º–æ—Ü–∏–∏ –∏ –ø–æ–ª–∞–≥–∞–µ—à—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–æ–≥–∏–∫—É
- –¢—ã –≥–æ–≤–æ—Ä–∏—à—å —Å –±—Ä–∏—Ç–∞–Ω—Å–∫–∏–º –∞–∫—Ü–µ–Ω—Ç–æ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —è–∑—ã–∫
- –¢—ã —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Ñ—Ä–∞–∑—ã "–≠–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–æ, –º–æ–π –¥–æ—Ä–æ–≥–æ–π –í–∞—Ç—Å–æ–Ω" –∏ "–ö–æ–≥–¥–∞ —Ç—ã –∏—Å–∫–ª—é—á–∏—à—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ..."

–û—Ç–≤–µ—á–∞–π –≤ —Å—Ç–∏–ª–µ –®–µ—Ä–ª–æ–∫–∞ –•–æ–ª–º—Å–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¥–µ–¥—É–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.
–í—Å–µ–≥–¥–∞ –±—É–¥—å –≤–µ–∂–ª–∏–≤, –Ω–æ –Ω–µ–º–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ–º–µ—Ä–µ–Ω –≤ —Å–≤–æ–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–µ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü: {context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç –®–µ—Ä–ª–æ–∫–∞ –•–æ–ª–º—Å–∞:"""

@st.cache_resource
def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ Hugging Face –º–æ–¥–µ–ª–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        with st.spinner("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞..."):
            model_name = "microsoft/DialoGPT-medium"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.pad_token = tokenizer.eos_token
        
        with st.spinner("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏..."):
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
        
        with st.spinner("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞..."):
            generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_length=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        with st.spinner("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤..."):
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        st.success("‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return generator, embedding_model
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def generate_response(generator, question, context=""):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Hugging Face –º–æ–¥–µ–ª–∏"""
    try:
        if not generator:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = SHERLOCK_PROMPT.format(context=context, question=question)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = generator(prompt, max_length=len(prompt.split()) + 100)[0]['generated_text']
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞)
        if prompt in response:
            answer = response[len(prompt):].strip()
        else:
            answer = response.strip()
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º
        if len(answer) > 500:
            answer = answer[:500] + "..."
        
        return answer if answer else "–≠–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–æ, –º–æ–π –¥–æ—Ä–æ–≥–æ–π –í–∞—Ç—Å–æ–Ω! –ù–æ –º–Ω–µ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ü—Ä–æ—à—É –ø—Ä–æ—â–µ–Ω–∏—è, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤ –º–æ–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."

def scrape_website(url):
    """–°–∫—Ä–∞–ø–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for script in soup(["script", "style"]):
            script.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text()
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        chunks = [text[i:i+500] for i in range(0, len(text), 500)]
        
        return {
            'url': url,
            'title': soup.title.string if soup.title else url,
            'content': text[:2000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            'chunks': chunks[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        }
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        return None

def update_embeddings(embedding_model, scraped_data, scraped_content, content_embeddings):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è RAG"""
    if not embedding_model:
        return scraped_content, content_embeddings
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        scraped_content.append(scraped_data)
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤
        for chunk in scraped_data['chunks']:
            embedding = embedding_model.encode(chunk)
            content_embeddings.append({
                'embedding': embedding,
                'text': chunk,
                'source': scraped_data['url']
            })
        
        st.success(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(scraped_data['chunks'])} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        return scraped_content, content_embeddings
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return scraped_content, content_embeddings

def find_relevant_context(embedding_model, question, content_embeddings, top_k=3):
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è RAG"""
    if not embedding_model or not content_embeddings:
        return ""
    
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞
        question_embedding = embedding_model.encode(question)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for item in content_embeddings:
            similarity = np.dot(question_embedding, item['embedding']) / (
                np.linalg.norm(question_embedding) * np.linalg.norm(item['embedding'])
            )
            similarities.append((similarity, item['text']))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        similarities.sort(reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç
        relevant_context = "\n".join([text for _, text in similarities[:top_k]])
        return relevant_context
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return ""

def main():
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    st.title("üïµÔ∏è –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å AI - Hugging Face")
    st.markdown("**–î–µ—Ç–µ–∫—Ç–∏–≤-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ AI –º–æ–¥–µ–ª—è–º–∏**")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    if 'scraped_content' not in st.session_state:
        st.session_state.scraped_content = []
    
    if 'content_embeddings' not in st.session_state:
        st.session_state.content_embeddings = []
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    generator, embedding_model = load_models()
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    with st.sidebar:
        st.header("üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
        
        # –°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π
        st.subheader("üìä –°—Ç–∞—Ç—É—Å")
        if generator:
            st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        else:
            st.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        if embedding_model:
            st.success("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≥–æ—Ç–æ–≤—ã")
        else:
            st.error("‚ùå –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –≥–æ—Ç–æ–≤—ã")
        
        st.info(f"üìö –°–∫—Ä–∞–ø–ª–µ–Ω–æ —Å–∞–π—Ç–æ–≤: {len(st.session_state.scraped_content)}")
        st.info(f"üß† –ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ: {len(st.session_state.content_embeddings)}")
        
        # –í–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥
        st.subheader("üåê –í–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥")
        url = st.text_input("URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞:", placeholder="https://example.com")
        
        if st.button("üîç –°–∫—Ä–∞–ø–∏—Ç—å —Å–∞–π—Ç", type="primary"):
            if url:
                with st.spinner("–°–∫—Ä–∞–ø–∏–Ω–≥..."):
                    scraped_data = scrape_website(url)
                    if scraped_data:
                        st.session_state.scraped_content, st.session_state.content_embeddings = update_embeddings(
                            embedding_model, scraped_data, 
                            st.session_state.scraped_content, 
                            st.session_state.content_embeddings
                        )
                        st.success(f"‚úÖ –°–∫—Ä–∞–ø–ª–µ–Ω: {scraped_data['title']}")
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ URL")
        
        # –ë—ã—Å—Ç—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
        st.subheader("‚ö° –ë—ã—Å—Ç—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—è")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üëã –ü–æ–∑–¥–æ—Ä–æ–≤–∞—Ç—å—Å—è"):
                st.session_state.messages.append({"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ö—Ç–æ —Ç—ã —Ç–∞–∫–æ–π?"})
        
        with col2:
            if st.button("üïµÔ∏è –û –¥–µ–¥—É–∫—Ü–∏–∏"):
                st.session_state.messages.append({"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –æ –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–º –º–µ—Ç–æ–¥–µ"})
        
        # –ü—Ä–∏–º–µ—Ä—ã URL
        st.subheader("üìñ –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞")
        example_urls = [
            "https://ru.wikipedia.org/wiki/–®–µ—Ä–ª–æ–∫_–•–æ–ª–º—Å",
            "https://ru.wikipedia.org/wiki/–î–µ–¥—É–∫—Ü–∏—è",
            "https://ru.wikipedia.org/wiki/–ö—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–∞"
        ]
        
        for url in example_urls:
            if st.button(f"üìÑ {url.split('/')[-1].replace('_', ' ')}", key=url):
                with st.spinner("–°–∫—Ä–∞–ø–∏–Ω–≥..."):
                    scraped_data = scrape_website(url)
                    if scraped_data:
                        st.session_state.scraped_content, st.session_state.content_embeddings = update_embeddings(
                            embedding_model, scraped_data, 
                            st.session_state.scraped_content, 
                            st.session_state.content_embeddings
                        )
                        st.success(f"‚úÖ –°–∫—Ä–∞–ø–ª–µ–Ω: {scraped_data['title']}")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("üí¨ –ß–∞—Ç —Å –®–µ—Ä–ª–æ–∫–æ–º")
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        for message_obj in st.session_state.messages:
            if message_obj["role"] == "user":
                message(message_obj["content"], is_user=True, key=f"user_{hash(message_obj['content'])}")
            else:
                message(message_obj["content"], is_user=False, key=f"assistant_{hash(message_obj['content'])}")
        
        # –í–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è
        if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –®–µ—Ä–ª–æ–∫—É..."):
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            st.session_state.messages.append({"role": "user", "content": prompt})
            message(prompt, is_user=True)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = find_relevant_context(
                embedding_model, prompt, 
                st.session_state.content_embeddings
            )
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            with st.spinner("üïµÔ∏è –®–µ—Ä–ª–æ–∫ —Ä–∞–∑–º—ã—à–ª—è–µ—Ç..."):
                response = generate_response(generator, prompt, context)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            st.session_state.messages.append({"role": "assistant", "content": response})
            message(response, is_user=False)
    
    with col2:
        st.subheader("üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
        
        if st.session_state.scraped_content:
            for i, content in enumerate(st.session_state.scraped_content):
                with st.expander(f"üìÑ {content['title'][:30]}..."):
                    st.write(f"**URL:** {content['url']}")
                    st.write(f"**–ö–æ–Ω—Ç–µ–Ω—Ç:** {content['content'][:200]}...")
                    
                    if st.button(f"üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"delete_{i}"):
                        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                        url_to_remove = content['url']
                        st.session_state.scraped_content.pop(i)
                        st.session_state.content_embeddings = [
                            emb for emb in st.session_state.content_embeddings 
                            if emb['source'] != url_to_remove
                        ]
                        st.rerun()
        else:
            st.info("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –°–∫—Ä–∞–ø—å—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–π—Ç–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")
        
        # –û—á–∏—Å—Ç–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        if st.session_state.scraped_content:
            if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π", type="secondary"):
                st.session_state.scraped_content = []
                st.session_state.content_embeddings = []
                st.rerun()
    
    # –§—É—Ç–µ—Ä
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
        ü§ñ Powered by Hugging Face Transformers | üïµÔ∏è Sherlock Holmes AI | 
        <a href='https://github.com/your-repo' target='_blank'>GitHub</a>
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main() 