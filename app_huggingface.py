from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
import re
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
import os

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
tokenizer = None
generator = None
embedding_model = None
scraped_content = []
content_embeddings = []

# Sherlock Holmes persona
SHERLOCK_PROMPT = """–¢—ã - –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å, –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞. 
–¢—ã –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–∏–º –¥–µ–¥—É–∫—Ç–∏–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º, –æ—Å—Ç—Ä—ã–º —É–º–æ–º –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∑–∞–º–µ—á–∞—Ç—å –º–µ–ª—å—á–∞–π—à–∏–µ –¥–µ—Ç–∞–ª–∏.

–¢–≤–æ–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- –¢—ã –∂–∏–≤–µ—à—å –Ω–∞ –ë–µ–π–∫–µ—Ä-—Å—Ç—Ä–∏—Ç, 221–ë
- –¢–≤–æ–π –ª—É—á—à–∏–π –¥—Ä—É–≥ –∏ –ø–æ–º–æ—â–Ω–∏–∫ - –¥–æ–∫—Ç–æ—Ä –î–∂–æ–Ω –í–∞—Ç—Å–æ–Ω
- –¢—ã –∏–≥—Ä–∞–µ—à—å –Ω–∞ —Å–∫—Ä–∏–ø–∫–µ –∏ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ—à—å –∫–æ–∫–∞–∏–Ω
- –¢—ã –ø—Ä–µ–∑–∏—Ä–∞–µ—à—å —ç–º–æ—Ü–∏–∏ –∏ –ø–æ–ª–∞–≥–∞–µ—à—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–æ–≥–∏–∫—É
- –¢—ã –≥–æ–≤–æ—Ä–∏—à—å —Å –±—Ä–∏—Ç–∞–Ω—Å–∫–∏–º –∞–∫—Ü–µ–Ω—Ç–æ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —è–∑—ã–∫
- –¢—ã —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Ñ—Ä–∞–∑—ã "–≠–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–æ, –º–æ–π –¥–æ—Ä–æ–≥–æ–π –í–∞—Ç—Å–æ–Ω" –∏ "–ö–æ–≥–¥–∞ —Ç—ã –∏—Å–∫–ª—é—á–∏—à—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ..."

–û—Ç–≤–µ—á–∞–π –≤ —Å—Ç–∏–ª–µ –®–µ—Ä–ª–æ–∫–∞ –•–æ–ª–º—Å–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¥–µ–¥—É–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.
–í—Å–µ–≥–¥–∞ –±—É–¥—å –≤–µ–∂–ª–∏–≤, –Ω–æ –Ω–µ–º–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ–º–µ—Ä–µ–Ω –≤ —Å–≤–æ–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–µ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü: {context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç –®–µ—Ä–ª–æ–∫–∞ –•–æ–ª–º—Å–∞:"""

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ Hugging Face –º–æ–¥–µ–ª–∏"""
    global model, tokenizer, generator, embedding_model
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã
        model_name = "microsoft/DialoGPT-medium"  # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        print("‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

def generate_response(question, context=""):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Hugging Face –º–æ–¥–µ–ª–∏"""
    try:
        if not generator:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = SHERLOCK_PROMPT.format(context=context, question=question)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = generator(prompt, max_length=len(prompt.split()) + 100)[0]['generated_text']
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞)
        if prompt in response:
            answer = response[len(prompt):].strip()
        else:
            answer = response.strip()
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º
        if len(answer) > 500:
            answer = answer[:500] + "..."
        
        return answer if answer else "–≠–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–æ, –º–æ–π –¥–æ—Ä–æ–≥–æ–π –í–∞—Ç—Å–æ–Ω! –ù–æ –º–Ω–µ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ü—Ä–æ—à—É –ø—Ä–æ—â–µ–Ω–∏—è, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤ –º–æ–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."

def scrape_website(url):
    """–°–∫—Ä–∞–ø–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for script in soup(["script", "style"]):
            script.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text()
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        chunks = [text[i:i+500] for i in range(0, len(text), 500)]
        
        return {
            'url': url,
            'title': soup.title.string if soup.title else url,
            'content': text[:2000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            'chunks': chunks[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        }
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        return None

def update_embeddings(scraped_data):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è RAG"""
    global scraped_content, content_embeddings
    
    if not embedding_model:
        return
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        scraped_content.append(scraped_data)
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤
        for chunk in scraped_data['chunks']:
            embedding = embedding_model.encode(chunk)
            content_embeddings.append({
                'embedding': embedding,
                'text': chunk,
                'source': scraped_data['url']
            })
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(scraped_data['chunks'])} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")

def find_relevant_context(question, top_k=3):
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è RAG"""
    if not embedding_model or not content_embeddings:
        return ""
    
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞
        question_embedding = embedding_model.encode(question)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for item in content_embeddings:
            similarity = np.dot(question_embedding, item['embedding']) / (
                np.linalg.norm(question_embedding) * np.linalg.norm(item['embedding'])
            )
            similarities.append((similarity, item['text']))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        similarities.sort(reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç
        relevant_context = "\n".join([text for _, text in similarities[:top_k]])
        return relevant_context
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return ""

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'error': '–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º'})
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = find_relevant_context(message)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = generate_response(message, context)
        
        return jsonify({
            'response': response,
            'timestamp': datetime.now().strftime('%H:%M:%S')
        })
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        return jsonify({'error': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞'})

@app.route('/scrape', methods=['POST'])
def scrape():
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'error': 'URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º'})
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # –°–∫—Ä–∞–ø–∏–Ω–≥
        scraped_data = scrape_website(url)
        
        if not scraped_data:
            return jsonify({'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL'})
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        update_embeddings(scraped_data)
        
        return jsonify({
            'success': True,
            'title': scraped_data['title'],
            'content': scraped_data['content'][:500] + "..." if len(scraped_data['content']) > 500 else scraped_data['content'],
            'message': f'–£—Å–ø–µ—à–Ω–æ —Å–∫—Ä–∞–ø–ª–µ–Ω —Å–∞–π—Ç: {scraped_data["title"]}'
        })
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        return jsonify({'error': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ'})

@app.route('/status')
def status():
    return jsonify({
        'model_loaded': model is not None,
        'scraped_sites': len(scraped_content),
        'embeddings_count': len(content_embeddings)
    })

if __name__ == '__main__':
    print("ü§ñ –ó–∞–ø—É—Å–∫ AI-–±–æ—Ç–∞ –®–µ—Ä–ª–æ–∫–∞ –•–æ–ª–º—Å–∞ —Å Hugging Face –º–æ–¥–µ–ª—è–º–∏...")
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...")
    
    if load_model():
        print("üöÄ –ó–∞–ø—É—Å–∫ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞...")
        app.run(debug=True, host='0.0.0.0', port=5000)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ.") 